# Data Ingestion Service

This service merges, cleans, de-duplicates, and ingests newline-delimited JSON (NDJSON) horse data into BigQuery via three Cloud Run Jobs.

## A. Structure

â€” **src/index.js**  
â€ƒLoads a cleaned master NDJSON from GCS into BigQuery:  
â€ƒâˆ™ stages into a transient table  
â€ƒâˆ™ performs a MERGE upsert into `horse_records`  
â€ƒâˆ™ sets `updated_at = CURRENT_TIMESTAMP()`  

â€” **src/cleanMaster.js**  
â€ƒStreams the master NDJSON from GCS, drops malformed JSON lines, removes duplicate `id` entries, and writes a cleaned/â€‹deduped file back to GCS  

â€” **src/mergeMaster.js** (if separate)  
â€ƒGathers all batch NDJSON files under `horse_data/`, concatenates them into a single `master_â€¦ndjson`, and deletes the original batch files  

â€” **Dockerfile**  
â€ƒBuilds a container image for all three jobs  

â€” **package.json**  
â€ƒCommonJS mode; dependencies on `@google-cloud/storage` and `@google-cloud/bigquery`

## B. Jobs & Usage

â€” **Merge Job**  
```powershell
gcloud beta run jobs create merge-job `
  --region=<REGION> `
  --image=gcr.io/<PROJECT_ID>/data-ingestion-service:latest `
  --command node `
  --args=src/mergeMaster.js,horse-racing-data-<PROJECT_ID> `
  --tasks=1 --memory=256Mi --task-timeout=600s
````

â€” **Clean & Dedupe Job**

```powershell
gcloud beta run jobs create clean-master-job `
  --region=<REGION> `
  --image=gcr.io/<PROJECT_ID>/data-ingestion-service:latest `
  --command node `
  --args=src/cleanMaster.js,horse-racing-data-<PROJECT_ID>,horse_data/master_*.ndjson `
  --tasks=1 --memory=512Mi --task-timeout=600s
```

â€” **Ingestion Job**

```powershell
gcloud beta run jobs create horse-ingestion-job `
  --region=<REGION> `
  --image=gcr.io/<PROJECT_ID>/data-ingestion-service:latest `
  --command node `
  --args=src/index.js,horse-racing-data-<PROJECT_ID>,horse_data/*.ndjson `
  --tasks=1 --memory=512Mi --task-timeout=600s
```

Replace `<PROJECT_ID>` and `<REGION>` as needed. Each jobâ€™s `--args` are comma-separated:
â€¢ Merge takes only the GCS bucket
â€¢ Clean takes bucket + master file pattern
â€¢ Ingest takes bucket + cleaned master file path

## C. Build & Push

```powershell
cd services/data_ingestion_service

docker build `
  --tag gcr.io/<PROJECT_ID>/data-ingestion-service:latest `
  .

docker push gcr.io/<PROJECT_ID>/data-ingestion-service:latest
```

*or via Cloud Build*

```powershell
gcloud builds submit `
  --tag gcr.io/<PROJECT_ID>/data-ingestion-service:latest
```

## D. Execution

â€” **Merge**

```powershell
gcloud beta run jobs execute merge-job --region=<REGION>
```

â€” **Clean**

```powershell
gcloud beta run jobs execute clean-master-job --region=<REGION>
```

â€” **Ingest**

```powershell
gcloud beta run jobs execute horse-ingestion-job --region=<REGION>
```

## E. Logs & Validation

â€” **Cloud Run Jobs â†’ Executions** shows each taskâ€™s stdout/stderr
â€” Key log entries:
â€ƒğŸ” â€œCleaning & de-duping master file: gs\://â€¦â€
â€ƒâœ… â€œClean & de-dupe complete. Kept X rowsâ€
â€ƒâ€œStarted load jobâ€¦â€
â€ƒâ€œMERGE completed successfully.â€

â€” **BigQuery row count**:

```bash
bq query --nouse_legacy_sql \
  'SELECT COUNT(*) FROM `horse-racing-predictor-<PROJECT_ID>.horse_racing_data.horse_records`'
```

Ensures final table matches your cleaned master fileâ€™s line count.