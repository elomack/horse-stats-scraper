// functions/orchestrator/index.js   (Node.js 20, 1st-gen Cloud Function)
// Orchestrates N scraper â†’ merge â†’ clean â†’ ingest batches.

import express from 'express';
import { Storage } from '@google-cloud/storage';
import { JobsClient } from '@google-cloud/run';
import axios from 'axios';

const app = express();

// â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
// Constants: project & region IDs, job/function names
// â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
const PROJECT_ID      = 'horse-racing-predictor-465217';
const REGION          = 'europe-central2';
const SCRAPER_JOB     = `projects/${PROJECT_ID}/locations/${REGION}/jobs/horse-data-scraper-job`;
const CLEAN_JOB       = `projects/${PROJECT_ID}/locations/${REGION}/jobs/clean-master-job`;
const INGEST_JOB      = `projects/${PROJECT_ID}/locations/${REGION}/jobs/horse-ingestion-job`;
const MERGE_URL       = `https://${REGION}-${PROJECT_ID}.cloudfunctions.net/mergeHorseData`;
const BUCKET_NAME     = 'horse-racing-data-elomack';
const CLEANED_PREFIX  = 'horse_data/';
const CLEANED_PATTERN = '_cleaned_deduped_';

const storage    = new Storage();
const jobsClient = new JobsClient();

app.get('/', async (req, res) => {
  console.log('ğŸ””  Orchestrator invoked');

  // â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  // 1) Parse & validate query params (multi-batch)
  // â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  const startId    = parseInt(req.query.startId,    10);
  const batchSize  = parseInt(req.query.batchSize,  10);
  const maxBatches = parseInt(req.query.maxBatches, 10);

  if ([startId, batchSize, maxBatches].some(n => !Number.isInteger(n) || n <= 0)) {
    return res
      .status(400)
      .send('startId, batchSize and maxBatches must all be positive integers');
  }
  console.log(`ğŸ›   Params: startId=${startId} batchSize=${batchSize} maxBatches=${maxBatches}`);

  try {
    // â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    // 2) Run scraper batches sequentially
    // â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    let currentStart = startId;
    const scrapeOps  = [];

    for (let batchNum = 1; batchNum <= maxBatches; batchNum++) {
      const endId = currentStart + batchSize - 1;
      console.log(`â¡ï¸  Batch ${batchNum}: IDs ${currentStart}-${endId}`);

      const [scrapeOp] = await jobsClient.runJob({
        name: SCRAPER_JOB,
        overrides: {
          containerOverrides: [
            { args: [ String(currentStart), String(batchSize) ] }
          ]
        }
      });
      console.log(`   â†ªï¸  Operation ${scrapeOp.name} started; waitingâ€¦`);
      await scrapeOp.promise();
      console.log(`âœ…  Batch ${batchNum} done`);

      scrapeOps.push(scrapeOp.name);
      currentStart += batchSize;
    }

    // â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    // 3) Merge partial NDJSON shards
    // â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    console.log('ğŸ”€  Invoking mergeHorseData Cloud Function');
    const mergeResp = await axios.post(MERGE_URL);
    if (!mergeResp.data?.masterFile) {
      throw new Error('mergeHorseData did not return masterFile');
    }
    const masterFile = mergeResp.data.masterFile;
    console.log(`âœ”ï¸  Merge complete: ${masterFile}`);

    // â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    // 4) Trigger clean-master-job and wait
    // â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    console.log('ğŸ§¹  Triggering clean-master-job');
    const [cleanOp] = await jobsClient.runJob({
      name: CLEAN_JOB,
      overrides: {
        containerOverrides: [
          {
            command: ["node", "src/cleanMaster.js"],
            env: [
              { name: "BUCKET_NAME", value: BUCKET_NAME },
              { name: "MASTER_FILE",  value: masterFile }
            ]
          }
        ]
      }
    });
    console.log(`   â†ªï¸  Clean job ${cleanOp.name} started; waitingâ€¦`);
    await cleanOp.promise();
    console.log('âœ…  Clean job done');

    // â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    // 5) List & pick the latest cleaned+deduped file
    // â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    console.log(`ğŸ“‹  Listing cleaned files under ${CLEANED_PREFIX}`);
    const [files] = await storage.bucket(BUCKET_NAME)
      .getFiles({ prefix: CLEANED_PREFIX });

    const cleanedFiles = files
      .map(f => f.name)
      .filter(name => name.includes(CLEANED_PATTERN))
      .sort();
    if (cleanedFiles.length === 0) {
      throw new Error('No cleaned files found');
    }
    const cleanedFile = cleanedFiles.pop();
    console.log(`âœ”ï¸  Picked cleaned file: ${cleanedFile}`);

    // â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    // 6) Trigger ingestion-job and wait
    // â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    console.log('ğŸ“¥  Triggering ingestion job');
    const [ingestOp] = await jobsClient.runJob({
      name: INGEST_JOB,
      overrides: {
        containerOverrides: [
          {
            command: ["node", "src/entrypoint.js"],
            args:    [ BUCKET_NAME, cleanedFile ]
          }
        ]
      }
    });
    console.log(`   â†ªï¸  Ingest job ${ingestOp.name} started; waitingâ€¦`);
    await ingestOp.promise();
    console.log('âœ…  Ingestion job done');

    // â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    // 7) Return consolidated summary
    // â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    res.status(200).json({
      startId,
      batchSize,
      maxBatches,
      masterFile,
      cleanedFile,
      operations: {
        scrapeBatches: scrapeOps,
        clean:         cleanOp.name,
        ingest:        ingestOp.name
      }
    });

  } catch (err) {
    console.error('âŒ  Orchestration error:', err);
    res.status(500).json({ error: err.message });
  }
});

// Export for Cloud Functions (1st-gen)
export const orchestrator = app;
