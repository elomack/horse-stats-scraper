# Orchestrator Cloud Function

A **first-generation HTTP-triggered** Cloud Function that runs one full scrape ‚Üí merge ‚Üí clean ‚Üí ingest **batch** in your pipeline.

---

## 1. What It Does

1. **Reads** `startId`, `batchSize` and `maxBatches` from the query string  
2. **Validates** they‚Äôre positive integers  
3. **Loops** (1‚Ä¶`maxBatches`):
   - Triggers the Cloud Run Job `horse-data-scraper-job` with `[startId, batchSize]`  
   - Waits for the job to finish  
   - Increments `startId += batchSize`
4. **Calls** the `mergeHorseData` Cloud Function (HTTP POST) to stitch shards into one master file  
5. **Triggers** the Cloud Run Job `clean-master-job` (via JobsClient) with env-vars:
   ```yaml
   BUCKET_NAME: horse-racing-data-elomack
   MASTER_FILE: <masterFile from merge>
````

6. **Lists** cleaned files in GCS, picks the newest one
7. **Triggers** the Cloud Run Job `horse-ingestion-job` with args `[BUCKET_NAME, cleanedFile]`
8. **Returns** a JSON summary:

```json
{
  "startId": 1,
  "batchSize": 1000,
  "maxBatches": 5,
  "masterFile": "horse_data/master_horse_data_2025-07-14T12_34_56_789Z.ndjson",
  "cleanedFile": "horse_data/master_horse_data_2025-07-14T12_35_10_123Z_cleaned_deduped.ndjson",
  "operations": {
    "scrape":  "projects/.../operations/abcd1234", 
    "merge":   "projects/.../locations/.../operations/efgh5678", 
    "clean":   "projects/.../operations/ijkl9012",
    "ingest":  "projects/.../operations/mnop3456"
  }
}
```

---

## 2. File Layout

```
functions/orchestrator/
‚îú‚îÄ‚îÄ index.js         # main Express app + orchestrator logic
‚îú‚îÄ‚îÄ package.json     # dependencies: @google-cloud/storage, @google-cloud/run, axios, express
‚îî‚îÄ‚îÄ README.md        # you are here
```

---

## 3. Configuration

* **No runtime env-vars**‚Äîall constants are hard-coded in `index.js`:

  * `PROJECT_ID`, `REGION`
  * Cloud Run Job names (`horse-data-scraper-job`, `clean-master-job`, `horse-ingestion-job`)
  * `MERGE_URL` ‚Üí your `mergeHorseData` URL
  * `BUCKET_NAME = horse-racing-data-elomack`
  * GCS prefixes / patterns

* **Query parameters** (all **required**):

  * `startId` -- integer ‚â• 1
  * `batchSize` -- integer ‚â• 1
  * `maxBatches` -- integer ‚â• 1

---

## 4. Deploying

```powershell
cd functions/orchestrator

npm install

gcloud functions deploy orchestrator `
  --source . `
  --region europe-central2 `
  --runtime nodejs20 `
  --trigger-http `
  --entry-point orchestrator `
  --timeout 300s `
  --allow-unauthenticated
```

---

## 5. Invocation

```bash
# via curl
curl -X GET \
  "https://REGION-PROJECT.cloudfunctions.net/orchestrator?startId=1&batchSize=1000&maxBatches=5"

# via gcloud
gcloud functions call orchestrator \
  --region=europe-central2 \
  --data '{}' \
  --parameters=startId=1,batchSize=1000,maxBatches=5
```

---

## 6. Logs & Troubleshooting

### View Logs

```bash
gcloud logging read \
  'resource.type="cloud_function"
   AND resource.labels.function_name="orchestrator"' \
  --limit 20 \
  --format="table(timestamp,severity,textPayload)"
```

### Key Log Lines

* `üîî  Orchestrator invoked`
* `üõ†  Params: startId=‚Ä¶ batchSize=‚Ä¶ maxBatches=‚Ä¶`
* `‚û°Ô∏è  Batch N: IDs ‚Ä¶` / `‚úÖ  Scraper batch done`
* `üîÄ  Invoking mergeHorseData Cloud Function`
* `‚úîÔ∏è  Merge complete ‚Ä¶`
* `üßπ  Triggering clean-master-job` / `‚úÖ  Clean job done`
* `üìã  Listing cleaned files ‚Ä¶` / `‚úîÔ∏è  Picked cleaned file ‚Ä¶`
* `üì•  Triggering ingestion job` / `‚úÖ  Ingestion job done`

If any step times out or throws, you‚Äôll see a stack trace in these logs‚Äîinspect the specific Cloud Run job or function that failed next.